{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "LLM's excel at a wide range of tasks, but they will struggle with queries specific to a unique business context. This is where Retrieval Augmented Generation (RAG) becomes invaluable. RAG enables an LLM to leverage your internal knowledge bases or customer support documents, significantly enhancing its ability to answer domain-specific questions. Enterprises are increasingly building RAG applications to improve workflows in customer support, Q&A over internal company documents, financial & legal analysis, and much more.\n",
    "\n",
    "This short notebook demonstrate how to create a simple RAG solution using the Anthropic documentation as the source for knowledge base.\n",
    "\n",
    "It creates up a basic RAG system using an in-memory vector database and embeddings from [Voyage AI](https://www.voyageai.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install needed libraries, including:\n",
    "\n",
    "1) `anthropic` - to interact with Claude\n",
    "\n",
    "2) `voyageai` - to generate high quality embeddings\n",
    "\n",
    "3) `pandas`, `numpy` - for data processing\n",
    "\n",
    "\n",
    "You'll also need an `API key` for:\n",
    "[Voyage AI](https://www.voyageai.com/) - for embeddings\n",
    "\n",
    "Optionally, provide an `API key` for Anthropic Cloud Service. If not provided the code will use Amazon Bedrock.\n",
    "For `API key` go to [Anthropic](https://www.anthropic.com/)\n",
    "\n",
    "Note: This code will run with Claude Haiku model unless changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Install the required python packages \n",
    "!pip install anthropic\n",
    "!pip install voyageai\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This notebook requires that at a minimum you have created an account \n",
    "# with VoyageAI (embeddings service) and have set the VOYAGE_API_KEY value in the\n",
    "# file that dotenv is going to read.\n",
    "# See the following for details: https://pypi.org/project/python-dotenv/\n",
    "\n",
    "# load_dotenv() loads in the key value information for the secret keys being used: \n",
    "\n",
    "# 1/ \"VOYAGE_API_KEY\"\n",
    "# 2/ \"ANTHROPIC_API_KEY\" - If you are using the service direct from Anthropic (rather than using Amazon Bedrock)\n",
    "\n",
    "load_dotenv()\n",
    "anthropic_api_key=os.getenv(\"ANTHROPIC_API_KEY\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a minimal in-Memory vector DB class\n",
    "\n",
    "In this example, we're using an in-memory vector DB, but for a production application, you may want to use a hosted solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import voyageai\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(self, name, api_key=None):\n",
    "        if api_key is None:\n",
    "            api_key = os.getenv(\"VOYAGE_API_KEY\")\n",
    "        self.client = voyageai.Client(api_key=api_key)\n",
    "        self.name = name\n",
    "        self.embeddings = []\n",
    "        self.metadata = []\n",
    "        self.query_cache = {}\n",
    "        self.db_path = f\"./data/{name}/vector_db.pkl\"\n",
    "\n",
    "    def load_data(self, data):\n",
    "        if self.embeddings and self.metadata:\n",
    "            print(\"Vector database is already loaded. Skipping data loading.\")\n",
    "            return\n",
    "        if os.path.exists(self.db_path):\n",
    "            print(\"Loading vector database from disk.\")\n",
    "            self.load_db()\n",
    "            return\n",
    "\n",
    "        texts = [f\"Heading: {item['chunk_heading']}\\n\\n Chunk Text:{item['text']}\" for item in data]\n",
    "        self._embed_and_store(texts, data)\n",
    "        self.save_db()\n",
    "        print(\"Vector database loaded and saved.\")\n",
    "\n",
    "    # TODO Change this function to limit text size sent to embeddeding to a max of 256 words\n",
    "    def _embed_and_store(self, texts, data):\n",
    "        batch_size = 128\n",
    "        result = [\n",
    "            self.client.embed(\n",
    "                texts[i : i + batch_size],\n",
    "                model=\"voyage-2\"\n",
    "            ).embeddings\n",
    "            for i in range(0, len(texts), batch_size)\n",
    "        ]\n",
    "        self.embeddings = [embedding for batch in result for embedding in batch]\n",
    "        self.metadata = data\n",
    "\n",
    "    def search(self, query, k=5, similarity_threshold=0.75):\n",
    "        if query in self.query_cache:\n",
    "            query_embedding = self.query_cache[query]\n",
    "        else:\n",
    "            query_embedding = self.client.embed([query], model=\"voyage-2\").embeddings[0]\n",
    "            self.query_cache[query] = query_embedding\n",
    "\n",
    "        if not self.embeddings:\n",
    "            raise ValueError(\"No data loaded in the vector database.\")\n",
    "\n",
    "        similarities = np.dot(self.embeddings, query_embedding)\n",
    "        top_indices = np.argsort(similarities)[::-1]\n",
    "        top_examples = []\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            if similarities[idx] >= similarity_threshold:\n",
    "                example = {\n",
    "                    \"metadata\": self.metadata[idx],\n",
    "                    \"similarity\": similarities[idx],\n",
    "                }\n",
    "                top_examples.append(example)\n",
    "                \n",
    "                if len(top_examples) >= k:\n",
    "                    break\n",
    "        return top_examples\n",
    "\n",
    "    def save_db(self):\n",
    "        data = {\n",
    "            \"embeddings\": self.embeddings,\n",
    "            \"metadata\": self.metadata,\n",
    "            \"query_cache\": json.dumps(self.query_cache),\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n",
    "        with open(self.db_path, \"wb\") as file:\n",
    "            pickle.dump(data, file)\n",
    "\n",
    "    def load_db(self):\n",
    "        if not os.path.exists(self.db_path):\n",
    "            raise ValueError(\"Vector database file not found. Use load_data to create a new database.\")\n",
    "        with open(self.db_path, \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "        self.embeddings = data[\"embeddings\"]\n",
    "        self.metadata = data[\"metadata\"]\n",
    "        self.query_cache = json.loads(data[\"query_cache\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1 - Basic RAG\n",
    "\n",
    "To get started, we'll set up a basic RAG pipeline using a bare bones approach. This is sometimes called 'Naive RAG' by many in the industry. A basic RAG pipeline includes the following 3 steps:\n",
    "\n",
    "1) Chunk documents by heading - containing only the content from each subheading\n",
    "\n",
    "2) Embed each document\n",
    "\n",
    "3) Use Cosine similarity to retrieve documents in order to answer query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the Anthropic documentation segments into a dictionary\n",
    "with open('data/anthropic_docs.json', 'r') as f:\n",
    "    anthropic_docs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(anthropic_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abbreviated_docs = anthropic_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abbreviated_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the VectorDB\n",
    "db = VectorDB(\"anthropic_docs\")\n",
    "# Import the document segments into the vector database\n",
    "db.load_data(abbreviated_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(db.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a minimal LLM Facade class\n",
    "\n",
    "This facade makes it easy to use either AWS Bedrock or Anthropic Cloud for invoking the LLM.\n",
    "If a value for the anthropic_api_key is set, then Anthropic Cloud will be used, otherwise, AWS Bedrock is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LLM_MAX_TOKENS = 2500\n",
    "LLM_TEMPERATURE = 0.01\n",
    "BEDROCK_MODEL_ID = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
    "\n",
    "\n",
    "class LlmFacade:\n",
    "    def __init__(self, anthropic_api_key=None):\n",
    "        self.max_tokens = LLM_MAX_TOKENS\n",
    "        self.temperature = LLM_TEMPERATURE\n",
    "        # Use Anthropic Claude via Anthropic Cloud if the key is set\n",
    "        # if not, set up to use Anthropic Claude via Bedrock\n",
    "        self.aws_bedrock = True\n",
    "\n",
    "        if anthropic_api_key:\n",
    "            self.anthropic_client = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "            self.aws_bedrock = False\n",
    "            print(\"Configured to use: Anthropic Cloud Service\")\n",
    "        else:\n",
    "            session = boto3.Session()\n",
    "            region = session.region_name\n",
    "\n",
    "            # Set the model id to Claude Haiku\n",
    "            self.bedrock_client = boto3.client(service_name='bedrock-runtime', region_name=region)\n",
    "            print(\"Configured to use: AWS Bedrock Service\")\n",
    "\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        if self.aws_bedrock == True:\n",
    "            return self.invoke_aws_bedrock_llm(prompt)\n",
    "        else:\n",
    "            return self.invoke_anthropic_cloud_llm(prompt)\n",
    "\n",
    "    def invoke_anthropic_cloud_llm(self, prompt: str) -> str:\n",
    "        messages = [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "\n",
    "        response = self.anthropic_client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=self.max_tokens,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=self.temperature\n",
    "        )\n",
    "        return response.content[0].text\n",
    "\n",
    "    def invoke_aws_bedrock_llm(self, prompt: str) -> str:\n",
    "        messages = [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "\n",
    "        inference_config = {\n",
    "            \"temperature\": self.temperature,\n",
    "            \"maxTokens\": self.max_tokens\n",
    "        }\n",
    "        converse_api_params = {\n",
    "            \"modelId\": BEDROCK_MODEL_ID,\n",
    "            \"messages\": messages,\n",
    "            \"inferenceConfig\": inference_config\n",
    "        }\n",
    "        # Send the request to the Bedrock service to generate a response\n",
    "        try:\n",
    "            response = self.bedrock_client.converse(**converse_api_params)\n",
    "\n",
    "            # Extract the generated text content from the response\n",
    "            text_content = response['output']['message']['content'][0]['text']\n",
    "\n",
    "            # Return the generated text content\n",
    "            return text_content\n",
    "\n",
    "        except ClientError as err:\n",
    "            message = err.response['Error']['Message']\n",
    "            print(f\"A client error occured: {message}\")\n",
    "        return(\"500: Request failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = LlmFacade(anthropic_api_key=anthropic_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm.invoke(\"how fast does a swallow fly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve_base(query, db, similarity_threshold=0.7):\n",
    "    results = db.search(query, k=3, similarity_threshold=similarity_threshold)\n",
    "    context = \"\"\n",
    "    for result in results:\n",
    "        chunk = result['metadata']\n",
    "        context += f\"\\n{chunk['text']}\\n\"\n",
    "    return results, context\n",
    "\n",
    "def answer_query_base(query, db, llm):\n",
    "    documents, context = retrieve_base(query, db)\n",
    "    prompt = f\"\"\"\n",
    "    You have been tasked with helping us to answer the following query: \n",
    "    <query>\n",
    "    {query}\n",
    "    </query>\n",
    "    You have access to the following documents which are meant to provide context as you answer the query:\n",
    "    <documents>\n",
    "    {context}\n",
    "    </documents>\n",
    "    Please remain faithful to the underlying context, and only deviate from it if you are 100% sure that you know the answer already. \n",
    "    Answer the question now, and avoid providing preamble such as 'Here is the answer', etc\n",
    "    \"\"\"\n",
    "    return llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_question = [\"i have a billing question\", \"what capabilities are there\", \"who's cat is that\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "results, context = retrieve_base(example_question[i], db)\n",
    "print(\"Question:\", example_question[i])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "results, context = retrieve_base(example_question[i], db, 0.7)\n",
    "print(\"Question:\", example_question[i])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 2\n",
    "results, context = retrieve_base(example_question[i], db, 0.7)\n",
    "print(\"Question:\", example_question[i])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "result = answer_query_base(example_question[i], db, llm)\n",
    "print(\"Question:\", example_question[i])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "result = answer_query_base(example_question[i], db, llm)\n",
    "print(\"Question:\", example_question[i])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 2\n",
    "result = answer_query_base(example_question[i], db, llm)\n",
    "print(\"Question:\", example_question[i])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
